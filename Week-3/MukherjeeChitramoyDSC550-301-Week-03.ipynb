{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a67975cf",
   "metadata": {},
   "source": [
    "#### DSC550-T301 \n",
    "##### Chitramoy Mukherjee\n",
    "##### Week-3\n",
    "##### Date: 12/11/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "263fb672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id  sentiment                                             review\n",
      "0  5814_8          1  With all this stuff going down at the moment w...\n",
      "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
      "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
      "3  3630_4          0  It must be assumed that those who praised this...\n",
      "4  9495_8          1  Superbly trashy and wondrously unpretentious 8...\n"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Required python basic libraries \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textblob           \n",
    "from textblob import TextBlob\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import download\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from os.path import basename, exists\n",
    "\n",
    "\n",
    "def download(url):\n",
    "    filename = basename(url)\n",
    "    if not exists(filename):\n",
    "        from urllib.request import urlretrieve\n",
    "\n",
    "        local, _ = urlretrieve(url, filename)\n",
    "        print(\"Downloaded \" + local)\n",
    "\n",
    "### Reading the labeledTrainData.tsv file into DataFrame\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\14024\\\\OneDrive\\\\Desktop\\\\MS-DSC\\\\DSC-550\\Week-3\\\\labeledTrainData.tsv\", sep='\\t', encoding='utf-8')\n",
    "\n",
    "# Display the first few rows of the DataFrame to ensure it's loaded properly\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1fbb4f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Positive Reviews: 12500\n",
      "Number of Negative Reviews: 12500\n"
     ]
    }
   ],
   "source": [
    "# How many of each positive and negative reviews are there?\n",
    "\n",
    "# Count the number of positive and negative reviews\n",
    "num_positive_reviews = df[df['sentiment'] == 1].shape[0]\n",
    "num_negative_reviews = df[df['sentiment'] == 0].shape[0]\n",
    "\n",
    "# Display the counts\n",
    "print(\"Number of Positive Reviews:\", num_positive_reviews)\n",
    "print(\"Number of Negative Reviews:\", num_negative_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4652538d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review predicted_sentiment\n",
      "0  With all this stuff going down at the moment w...            positive\n",
      "1  \\The Classic War of the Worlds\\\" by Timothy Hi...            positive\n",
      "2  The film starts with a manager (Nicholas Bell)...            negative\n",
      "3  It must be assumed that those who praised this...            positive\n",
      "4  Superbly trashy and wondrously unpretentious 8...            negative\n"
     ]
    }
   ],
   "source": [
    "# Use TextBlob to classify each movie review as positive or negative. Assume that a polarity score greater than or equal to zero is a positive sentiment and less than 0 is a negative sentiment.\n",
    "def classify_sentiment(review):\n",
    "    analysis = TextBlob(review)\n",
    "    return 'positive' if analysis.polarity >= 0 else 'negative'\n",
    "\n",
    "# Apply the sentiment classification to the 'review' column\n",
    "df['predicted_sentiment'] = df['review'].apply(classify_sentiment)\n",
    "\n",
    "# Display the DataFrame with the predicted sentiment\n",
    "print(df[['review', 'predicted_sentiment']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3ab2a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment  \\\n",
      "0  With all this stuff going down at the moment w...          1   \n",
      "1  \\The Classic War of the Worlds\\\" by Timothy Hi...          1   \n",
      "2  The film starts with a manager (Nicholas Bell)...          0   \n",
      "3  It must be assumed that those who praised this...          0   \n",
      "4  Superbly trashy and wondrously unpretentious 8...          1   \n",
      "\n",
      "   predicted_sentiment  \n",
      "0                    1  \n",
      "1                    1  \n",
      "2                    0  \n",
      "3                    1  \n",
      "4                    0  \n",
      "Accuracy: 68.52%\n",
      "Random Guessing Accuracy: 50.00%\n",
      "The sentiment analysis model is better than random guessing.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Function to classify sentiment using TextBlob\n",
    "def classify_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return 1 if analysis.sentiment.polarity >= 0 else 0\n",
    "\n",
    "# Apply sentiment classification to each review\n",
    "df['predicted_sentiment'] = df['review'].apply(classify_sentiment)\n",
    "print(df[['review', 'sentiment', 'predicted_sentiment']].head())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(df['sentiment'], df['predicted_sentiment'])\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Compare with random guessing accuracy\n",
    "random_guessing_accuracy = max(df['sentiment'].mean(), 1 - df['sentiment'].mean())\n",
    "print(f'Random Guessing Accuracy: {random_guessing_accuracy * 100:.2f}%')\n",
    "\n",
    "# Compare with random guessing\n",
    "if accuracy > random_guessing_accuracy:\n",
    "    print(\"The sentiment analysis model is better than random guessing.\")\n",
    "else:\n",
    "    print(\"The sentiment analysis model is not better than random guessing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51f04163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flairNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading flair-0.13.0-py3-none-any.whl (387 kB)\n",
      "     -------------------------------------- 387.2/387.2 kB 4.0 MB/s eta 0:00:00\n",
      "Collecting segtok>=1.5.11\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Collecting gensim>=4.2.0\n",
      "  Downloading gensim-4.3.2-cp39-cp39-win_amd64.whl (24.0 MB)\n",
      "     --------------------------------------- 24.0/24.0 MB 10.7 MB/s eta 0:00:00\n",
      "Collecting langdetect>=1.0.9\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     -------------------------------------- 981.5/981.5 kB 4.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting wikipedia-api>=0.5.7\n",
      "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
      "Collecting gdown>=4.4.0\n",
      "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n",
      "Collecting conllu>=4.0\n",
      "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
      "Collecting ftfy>=6.1.0\n",
      "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
      "     ---------------------------------------- 53.4/53.4 kB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: lxml>=4.8.0 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from flair) (4.9.1)\n",
      "Collecting deprecated>=1.2.13\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting transformer-smaller-training-vocab>=0.2.3\n",
      "  Downloading transformer_smaller_training_vocab-0.3.3-py3-none-any.whl (14 kB)\n",
      "Collecting mpld3>=0.3\n",
      "  Downloading mpld3-0.5.9-py3-none-any.whl (201 kB)\n",
      "     -------------------------------------- 201.2/201.2 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.63.0 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from flair) (4.64.1)\n",
      "Collecting semver<4.0.0,>=3.0.0\n",
      "  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
      "Collecting sqlitedict>=2.0.0\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from flair) (2.8.2)\n",
      "Collecting huggingface-hub>=0.10.0\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "     -------------------------------------- 311.7/311.7 kB 9.7 MB/s eta 0:00:00\n",
      "Collecting torch!=1.8,>=1.5.0\n",
      "  Downloading torch-2.1.2-cp39-cp39-win_amd64.whl (192.2 MB)\n",
      "     -------------------------------------- 192.2/192.2 MB 4.1 MB/s eta 0:00:00\n",
      "Collecting bpemb>=0.3.2\n",
      "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.10 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from flair) (0.8.10)\n",
      "Requirement already satisfied: boto3>=1.20.27 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from flair) (1.24.28)\n",
      "Collecting more-itertools>=8.13.0\n",
      "  Downloading more_itertools-10.1.0-py3-none-any.whl (55 kB)\n",
      "     ---------------------------------------- 55.8/55.8 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting pptree>=3.1\n",
      "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from flair) (2022.7.9)\n",
      "Collecting transformers[sentencepiece]<5.0.0,>=4.18.0\n",
      "  Downloading transformers-4.36.1-py3-none-any.whl (8.3 MB)\n",
      "     ---------------------------------------- 8.3/8.3 MB 3.1 MB/s eta 0:00:00\n",
      "Collecting pytorch-revgrad>=0.2.0\n",
      "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from flair) (1.0.2)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.0.0 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from flair) (1.26.11)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from flair) (3.5.2)\n",
      "Collecting janome>=0.4.2\n",
      "  Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
      "     ---------------------------------------- 19.7/19.7 MB 4.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from boto3>=1.20.27->flair) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from boto3>=1.20.27->flair) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.28 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from boto3>=1.20.27->flair) (1.27.28)\n",
      "Requirement already satisfied: numpy in c:\\users\\14024\\anaconda3\\lib\\site-packages (from bpemb>=0.3.2->flair) (1.21.5)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp39-cp39-win_amd64.whl (977 kB)\n",
      "     -------------------------------------- 977.6/977.6 kB 3.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\14024\\anaconda3\\lib\\site-packages (from bpemb>=0.3.2->flair) (2.28.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from deprecated>=1.2.13->flair) (1.14.1)\n",
      "Collecting wcwidth<0.3.0,>=0.2.12\n",
      "  Downloading wcwidth-0.2.12-py2.py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from gdown>=4.4.0->flair) (4.11.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\14024\\anaconda3\\lib\\site-packages (from gdown>=4.4.0->flair) (3.6.0)\n",
      "Requirement already satisfied: six in c:\\users\\14024\\anaconda3\\lib\\site-packages (from gdown>=4.4.0->flair) (1.16.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from gensim>=4.2.0->flair) (1.9.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from gensim>=4.2.0->flair) (5.2.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (21.3)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "     -------------------------------------- 169.0/169.0 kB 9.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (4.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (6.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from mpld3>=0.3->flair) (2.11.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.2->flair) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.2->flair) (1.1.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\14024\\anaconda3\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\14024\\anaconda3\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (2.8.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\14024\\anaconda3\\lib\\site-packages (from tqdm>=4.63.0->flair) (0.4.5)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 3.2 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.1-cp39-none-win_amd64.whl (277 kB)\n",
      "     -------------------------------------- 277.8/277.8 kB 4.3 MB/s eta 0:00:00\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl (413 kB)\n",
      "     -------------------------------------- 413.4/413.4 kB 5.2 MB/s eta 0:00:00\n",
      "Collecting accelerate>=0.21.0\n",
      "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
      "     -------------------------------------- 265.7/265.7 kB 5.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from jinja2->mpld3>=0.3->flair) (2.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.3.2->flair) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.3.2->flair) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.3.2->flair) (2022.9.14)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\14024\\anaconda3\\lib\\site-packages (from sympy->torch!=1.8,>=1.5.0->flair) (1.2.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\14024\\anaconda3\\lib\\site-packages (from accelerate>=0.21.0->transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (5.9.0)\n",
      "Building wheels for collected packages: langdetect, pptree, sqlitedict\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=a3cf388b0018e2c805c2bff2f118cf0dfa21e61845ebc3aa5941a682da4c3eda\n",
      "  Stored in directory: c:\\users\\14024\\appdata\\local\\pip\\cache\\wheels\\d1\\c1\\d9\\7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
      "  Building wheel for pptree (setup.py): started\n",
      "  Building wheel for pptree (setup.py): finished with status 'done'\n",
      "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4609 sha256=e383d8cf4a836ff0d4f3af95a8db4e98a2f494a56052a6475385445f626720f0\n",
      "  Stored in directory: c:\\users\\14024\\appdata\\local\\pip\\cache\\wheels\\52\\0e\\51\\514e690004ea9713bc3fdb678d5e2768fcc597d0c3b6a3abd2\n",
      "  Building wheel for sqlitedict (setup.py): started\n",
      "  Building wheel for sqlitedict (setup.py): finished with status 'done'\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=3ff2797b84a62ba7be9bc9982b1675bc06fb76f749b7d6aedd62b8ab06b8a17c\n",
      "  Stored in directory: c:\\users\\14024\\appdata\\local\\pip\\cache\\wheels\\f6\\48\\c4\\942f7a1d556fddd2348cb9ac262f251873dfd8a39afec5678e\n",
      "Successfully built langdetect pptree sqlitedict\n",
      "Installing collected packages: wcwidth, sqlitedict, sentencepiece, pptree, janome, semver, segtok, safetensors, protobuf, more-itertools, langdetect, ftfy, fsspec, deprecated, conllu, wikipedia-api, torch, huggingface-hub, gensim, tokenizers, pytorch-revgrad, mpld3, gdown, bpemb, accelerate, transformers, transformer-smaller-training-vocab, flair\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.5\n",
      "    Uninstalling wcwidth-0.2.5:\n",
      "      Successfully uninstalled wcwidth-0.2.5\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.7.1\n",
      "    Uninstalling fsspec-2022.7.1:\n",
      "      Successfully uninstalled fsspec-2022.7.1\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 4.1.2\n",
      "    Uninstalling gensim-4.1.2:\n",
      "      Successfully uninstalled gensim-4.1.2\n",
      "Successfully installed accelerate-0.25.0 bpemb-0.3.4 conllu-4.5.3 deprecated-1.2.14 flair-0.13.0 fsspec-2023.12.2 ftfy-6.1.3 gdown-4.7.1 gensim-4.3.2 huggingface-hub-0.19.4 janome-0.5.0 langdetect-1.0.9 more-itertools-10.1.0 mpld3-0.5.9 pptree-3.1 protobuf-4.25.1 pytorch-revgrad-0.2.0 safetensors-0.4.1 segtok-1.5.11 semver-3.0.2 sentencepiece-0.1.99 sqlitedict-2.1.0 tokenizers-0.15.0 torch-2.1.2 transformer-smaller-training-vocab-0.3.3 transformers-4.36.1 wcwidth-0.2.12 wikipedia-api-0.6.0\n"
     ]
    }
   ],
   "source": [
    "pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c9df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize the Flair text classifier (pre-trained model for sentiment analysis)\n",
    "classifier = TextClassifier.load('en-sentiment')\n",
    "\n",
    "# Classify each review using Flair\n",
    "def classify_with_flair(review):\n",
    "    sentence = Sentence(review)\n",
    "    classifier.predict(sentence)\n",
    "    return sentence.labels[0].value.lower()\n",
    "\n",
    "df['Flair_Prediction'] = df['review'].apply(classify_with_flair)\n",
    "\n",
    "# Map positive and negative labels to 1 and 0 for comparison\n",
    "df['Flair_Prediction'] = df['Flair_Prediction'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Check the accuracy of the Flair model\n",
    "accuracy = accuracy_score(df['sentiment'], df['Flair_Prediction'])\n",
    "print(\"\\nAccuracy of the Flair model: {:.2%}\".format(accuracy))\n",
    "\n",
    "# Compare with random guessing\n",
    "random_accuracy = 0.5  # Assuming a binary classification\n",
    "print(\"Accuracy of random guessing: {:.2%}\".format(random_accuracy))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d5095a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows of the DataFrame after converting to lowercase:\n",
      "       id  sentiment                                             review  \\\n",
      "0  5814_8          1  With all this stuff going down at the moment w...   \n",
      "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...   \n",
      "2  7759_3          0  The film starts with a manager (Nicholas Bell)...   \n",
      "3  3630_4          0  It must be assumed that those who praised this...   \n",
      "4  9495_8          1  Superbly trashy and wondrously unpretentious 8...   \n",
      "\n",
      "  predicted_sentiment                                             Review  \n",
      "0            positive  with all this stuff going down at the moment w...  \n",
      "1            positive  \\the classic war of the worlds\\\" by timothy hi...  \n",
      "2            negative  the film starts with a manager (nicholas bell)...  \n",
      "3            positive  it must be assumed that those who praised this...  \n",
      "4            negative  superbly trashy and wondrously unpretentious 8...  \n"
     ]
    }
   ],
   "source": [
    "# Convert all text to lowercase\n",
    "df['Review'] = df['review'].str.lower()\n",
    "\n",
    "# Display the first few rows after converting to lowercase\n",
    "print(\"\\nFirst few rows of the DataFrame after converting to lowercase:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d61d54f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame with cleaned reviews:\n",
      "                                              review  \\\n",
      "0  With all this stuff going down at the moment w...   \n",
      "1  \\The Classic War of the Worlds\\\" by Timothy Hi...   \n",
      "2  The film starts with a manager (Nicholas Bell)...   \n",
      "3  It must be assumed that those who praised this...   \n",
      "4  Superbly trashy and wondrously unpretentious 8...   \n",
      "\n",
      "                                      Cleaned_review  \n",
      "0  With all this stuff going down at the moment w...  \n",
      "1  The Classic War of the Worlds by Timothy Hines...  \n",
      "2  The film starts with a manager Nicholas Bell g...  \n",
      "3  It must be assumed that those who praised this...  \n",
      "4  Superbly trashy and wondrously unpretentious 8...  \n"
     ]
    }
   ],
   "source": [
    "# Function to remove punctuation and special characters\n",
    "def remove_punctuation(text):\n",
    "    # Use string.punctuation to get the set of all punctuation characters\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    # Remove punctuation using the translator\n",
    "    text_no_punct = text.translate(translator)\n",
    "    return text_no_punct\n",
    "\n",
    "# Remove punctuation and special characters from the 'Review' column\n",
    "df['Cleaned_review'] = df['review'].apply(remove_punctuation)\n",
    "\n",
    "# Display the DataFrame with the cleaned reviews\n",
    "print(\"\\nDataFrame with cleaned reviews:\")\n",
    "print(df[['review', 'Cleaned_review']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62c9901f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows of the DataFrame with cleaned reviews:\n",
      "                                              review  \\\n",
      "0  With all this stuff going down at the moment w...   \n",
      "1  \\The Classic War of the Worlds\\\" by Timothy Hi...   \n",
      "2  The film starts with a manager (Nicholas Bell)...   \n",
      "3  It must be assumed that those who praised this...   \n",
      "4  Superbly trashy and wondrously unpretentious 8...   \n",
      "\n",
      "                                      Cleaned_review  \n",
      "0  stuff going moment mj started listening music ...  \n",
      "1  classic war timothy hines entertaining film ob...  \n",
      "2  film starts manager nicholas bell giving welco...  \n",
      "3  must assumed praised film greatest filmed oper...  \n",
      "4  superbly trashy wondrously unpretentious 80 ex...  \n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "df['Cleaned_review'] = df['review'].apply(remove_stop_words)\n",
    "\n",
    "# Display the first few rows of the DataFrame with cleaned reviews\n",
    "print(\"\\nFirst few rows of the DataFrame with cleaned reviews:\")\n",
    "print(df[['review', 'Cleaned_review']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b640cadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame with Stemmed Reviews:\n",
      "                                                  review  \\\n",
      "0      With all this stuff going down at the moment w...   \n",
      "1      \\The Classic War of the Worlds\\\" by Timothy Hi...   \n",
      "2      The film starts with a manager (Nicholas Bell)...   \n",
      "3      It must be assumed that those who praised this...   \n",
      "4      Superbly trashy and wondrously unpretentious 8...   \n",
      "...                                                  ...   \n",
      "24995  It seems like more consideration has gone into...   \n",
      "24996  I don't believe they made this film. Completel...   \n",
      "24997  Guy is a loser. Can't get girls, needs to buil...   \n",
      "24998  This 30 minute documentary Buñuel made in the ...   \n",
      "24999  I saw this movie as a child and it broke my he...   \n",
      "\n",
      "                                          Stemmed_review  \n",
      "0      with all thi stuff go down at the moment with ...  \n",
      "1      \\the classic war of the worlds\\ '' by timothi ...  \n",
      "2      the film start with a manag ( nichola bell ) g...  \n",
      "3      it must be assum that those who prais thi film...  \n",
      "4      superbl trashi and wondrous unpretenti 80 's e...  \n",
      "...                                                  ...  \n",
      "24995  it seem like more consider ha gone into the im...  \n",
      "24996  i do n't believ they made thi film . complet u...  \n",
      "24997  guy is a loser . ca n't get girl , need to bui...  \n",
      "24998  thi 30 minut documentari buñuel made in the ea...  \n",
      "24999  i saw thi movi as a child and it broke my hear...  \n",
      "\n",
      "[25000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Apply PorterStemmer to each review\n",
    "df['Stemmed_review'] = df['review'].apply(lambda x: ' '.join([porter.stem(word) for word in word_tokenize(x)]))\n",
    "\n",
    "# Display the DataFrame with the new 'Stemmed_Review' column\n",
    "print(\"\\nDataFrame with Stemmed Reviews:\")\n",
    "print(df[['review', 'Stemmed_review']])\n",
    "\n",
    "# Save the DataFrame with the stemmed reviews if needed\n",
    "# df.to_csv('path_to_save_stemmed_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5705447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of the bag-of-words matrix:\n",
      "Number of Rows (Reviews): 25000\n",
      "Number of Columns (Unique Words): 59685\n"
     ]
    }
   ],
   "source": [
    "# Create a bag-of-words matrix from your stemmed text (output from (4)), where each row is a word-count vector for a single movie review\n",
    "# Initialize NLTK's PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Tokenize and apply stemming to each review\n",
    "df['Stemmed_Review'] = df['review'].apply(lambda x: ' '.join([porter_stemmer.stem(word) for word in word_tokenize(x)]))\n",
    "\n",
    "# Create a bag-of-words matrix using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "bag_of_words_matrix = vectorizer.fit_transform(df['Stemmed_review'])\n",
    "\n",
    "# Display the dimensions of the bag-of-words matrix\n",
    "print(\"\\nDimensions of the bag-of-words matrix:\")\n",
    "print(\"Number of Rows (Reviews):\", bag_of_words_matrix.shape[0])\n",
    "print(\"Number of Columns (Unique Words):\", bag_of_words_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb2115bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\14024\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of the bag-of-words matrix:\n",
      "Rows (documents): 25000\n",
      "Columns (unique words): 59685\n",
      "\n",
      "Dimensions of the tf-idf matrix:\n",
      "Rows (documents): 25000\n",
      "Columns (unique words): 59685\n"
     ]
    }
   ],
   "source": [
    "# Create a term frequency-inverse document frequency (tf-idf) matrix from your stemmed text, for your movie review\n",
    "# Apply NLTK's PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "df['Stemmed_review'] = df['review'].apply(lambda x: ' '.join([porter.stem(word) for word in word_tokenize(x)]))\n",
    "\n",
    "# Create a bag-of-words matrix\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(df['Stemmed_review'])\n",
    "\n",
    "# Display dimensions of the bag-of-words matrix\n",
    "print(\"\\nDimensions of the bag-of-words matrix:\")\n",
    "print(\"Rows (documents):\", bow_matrix.shape[0])\n",
    "print(\"Columns (unique words):\", bow_matrix.shape[1])\n",
    "\n",
    "# Create a tf-idf matrix\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Stemmed_review'])\n",
    "\n",
    "# Display dimensions of the tf-idf matrix\n",
    "print(\"\\nDimensions of the tf-idf matrix:\")\n",
    "print(\"Rows (documents):\", tfidf_matrix.shape[0])\n",
    "print(\"Columns (unique words):\", tfidf_matrix.shape[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
